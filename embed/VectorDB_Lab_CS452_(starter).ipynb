{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iftt-jonathan/database-experiment-colabs/blob/main/embed/VectorDB_Lab_CS452_(starter).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4UOUNsUTsvcn",
        "outputId": "084da63e-76be-4a63-b786-38b4154a1e47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Dataset URL: https://www.kaggle.com/datasets/michaeltreynolds/lex-fridman-text-embedding-3-large-128\n",
            "License(s): MIT\n",
            "Downloading lex-fridman-text-embedding-3-large-128.zip to /content\n",
            " 99% 586M/594M [00:01<00:00, 260MB/s]\n",
            "100% 594M/594M [00:01<00:00, 324MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download datasets from kaggle\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "if not os.path.exists(\"lex-fridman-text-embedding-3-large-128.zip\"):\n",
        "  kaggle_json = {\"username\": \"michaeltreynolds\",\"key\": \"149701be742f30a8a0526762c61beea0\"}\n",
        "  kaggle_dir = os.path.join(os.path.expanduser(\"~\"), \".kaggle\")\n",
        "  os.makedirs(kaggle_dir, exist_ok=True)\n",
        "  kaggle_config_path = os.path.join(kaggle_dir, \"kaggle.json\")\n",
        "  with open(kaggle_config_path, 'w') as f:\n",
        "    json.dump(kaggle_json, f)\n",
        "\n",
        "  !kaggle datasets download -d michaeltreynolds/lex-fridman-text-embedding-3-large-128\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip kaggle data\n",
        "\n",
        "!unzip lex-fridman-text-embedding-3-large-128.zip\n",
        "!unzip lex-fridman-text-embedding-3-large-128/*.zip\n"
      ],
      "metadata": {
        "id": "h3swnD70x4FG",
        "outputId": "dca5de7b-169c-4abf-936e-711fd25f3a02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  lex-fridman-text-embedding-3-large-128.zip\n",
            "  inflating: documents/documents/batch_request_0lw3vrQqdWbdBRurTGNMHU76.jsonl  \n",
            "  inflating: documents/documents/batch_request_3GozevpriRRzieX4za9xfNmY.jsonl  \n",
            "  inflating: documents/documents/batch_request_3maYxwEF1svWRpbYr10br6Wv.jsonl  \n",
            "  inflating: documents/documents/batch_request_7hQA9m3pUXx22JXInjYZNAu2.jsonl  \n",
            "  inflating: documents/documents/batch_request_7oR3UbWsHgESFLr5eqL3jkMD.jsonl  \n",
            "  inflating: documents/documents/batch_request_CXU6VbN4SDinplgj1MpILc1u.jsonl  \n",
            "  inflating: documents/documents/batch_request_Fve0NjohSf5Qe0bZtAnp8D5A.jsonl  \n",
            "  inflating: documents/documents/batch_request_If8QibqlgU0XRU5FcD5zpiuL.jsonl  \n",
            "  inflating: documents/documents/batch_request_KQAdZBdQHYMI2MfEMXf3ytLM.jsonl  \n",
            "  inflating: documents/documents/batch_request_MFMHpnaCSkNrbgAgOxCzaLOl.jsonl  \n",
            "  inflating: documents/documents/batch_request_NDF2wpJfxtprLcfkIUYE2iWQ.jsonl  \n",
            "  inflating: documents/documents/batch_request_S6oh8W0n2tNadcBvYxOzzYNx.jsonl  \n",
            "  inflating: documents/documents/batch_request_WsWMgjQhn3wXtMZcXuKc1ZE7.jsonl  \n",
            "  inflating: documents/documents/batch_request_Z6MhxEvYsKphmnJTno6L6QkW.jsonl  \n",
            "  inflating: documents/documents/batch_request_gWb7SDYIzTMTN4plMW2auahA.jsonl  \n",
            "  inflating: documents/documents/batch_request_hE3eSd3c5AQWcMWpaV0dBJPh.jsonl  \n",
            "  inflating: documents/documents/batch_request_n6Q1PS1f6wiNnWJd6qzIcbKf.jsonl  \n",
            "  inflating: embedding/embedding/0lw3vrQqdWbdBRurTGNMHU76.jsonl  \n",
            "  inflating: embedding/embedding/3GozevpriRRzieX4za9xfNmY.jsonl  \n",
            "  inflating: embedding/embedding/3maYxwEF1svWRpbYr10br6Wv.jsonl  \n",
            "  inflating: embedding/embedding/7hQA9m3pUXx22JXInjYZNAu2.jsonl  \n",
            "  inflating: embedding/embedding/7oR3UbWsHgESFLr5eqL3jkMD.jsonl  \n",
            "  inflating: embedding/embedding/CXU6VbN4SDinplgj1MpILc1u.jsonl  \n",
            "  inflating: embedding/embedding/Fve0NjohSf5Qe0bZtAnp8D5A.jsonl  \n",
            "  inflating: embedding/embedding/If8QibqlgU0XRU5FcD5zpiuL.jsonl  \n",
            "  inflating: embedding/embedding/KQAdZBdQHYMI2MfEMXf3ytLM.jsonl  \n",
            "  inflating: embedding/embedding/MFMHpnaCSkNrbgAgOxCzaLOl.jsonl  \n",
            "  inflating: embedding/embedding/NDF2wpJfxtprLcfkIUYE2iWQ.jsonl  \n",
            "  inflating: embedding/embedding/S6oh8W0n2tNadcBvYxOzzYNx.jsonl  \n",
            "  inflating: embedding/embedding/WsWMgjQhn3wXtMZcXuKc1ZE7.jsonl  \n",
            "  inflating: embedding/embedding/Z6MhxEvYsKphmnJTno6L6QkW.jsonl  \n",
            "  inflating: embedding/embedding/gWb7SDYIzTMTN4plMW2auahA.jsonl  \n",
            "  inflating: embedding/embedding/hE3eSd3c5AQWcMWpaV0dBJPh.jsonl  \n",
            "  inflating: embedding/embedding/n6Q1PS1f6wiNnWJd6qzIcbKf.jsonl  \n",
            "unzip:  cannot find or open lex-fridman-text-embedding-3-large-128/*.zip, lex-fridman-text-embedding-3-large-128/*.zip.zip or lex-fridman-text-embedding-3-large-128/*.zip.ZIP.\n",
            "\n",
            "No zipfiles found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use specific libraries\n",
        "!pip install datasets==2.20.0 psycopg2==2.9.9 pgcopy==1.6.0\n",
        "import psycopg2"
      ],
      "metadata": {
        "id": "SYDFzWfv4HLW",
        "outputId": "f880de55-efe0-4829-9084-42a0818fdc32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets==2.20.0\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting psycopg2==2.9.9\n",
            "  Downloading psycopg2-2.9.9.tar.gz (384 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.9/384.9 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pgcopy==1.6.0\n",
            "  Downloading pgcopy-1.6.0-py2.py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (18.1.0)\n",
            "Collecting pyarrow-hotfix (from datasets==2.20.0)\n",
            "  Downloading pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (0.70.15)\n",
            "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets==2.20.0)\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (0.31.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (6.0.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from pgcopy==1.6.0) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.2->datasets==2.20.0) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==2.20.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==2.20.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==2.20.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==2.20.0) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.20.0) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.20.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.20.0) (1.17.0)\n",
            "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pgcopy-1.6.0-py2.py3-none-any.whl (13 kB)\n",
            "Downloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\n",
            "Building wheels for collected packages: psycopg2\n",
            "  Building wheel for psycopg2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for psycopg2: filename=psycopg2-2.9.9-cp311-cp311-linux_x86_64.whl size=508998 sha256=3f834954a6894d3004f1628557d063220b5744c7e11f134b5bed13d1d0d86640\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/34/b9/78ebef1b3220b4840ee482461e738566c3c9165d2b5c914f51\n",
            "Successfully built psycopg2\n",
            "Installing collected packages: pyarrow-hotfix, psycopg2, fsspec, pgcopy, datasets\n",
            "  Attempting uninstall: psycopg2\n",
            "    Found existing installation: psycopg2 2.9.10\n",
            "    Uninstalling psycopg2-2.9.10:\n",
            "      Successfully uninstalled psycopg2-2.9.10\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 fsspec-2024.5.0 pgcopy-1.6.0 psycopg2-2.9.9 pyarrow-hotfix-0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get your own trial account at timescaledb and paste your own connection string\n",
        "\n",
        "#TODO\n",
        "CONNECTION = \"<Connection string here>\""
      ],
      "metadata": {
        "id": "ukT4dY-z25XG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use this if you want to start over on your postgres table!\n",
        "\n",
        "DROP_TABLE = \"DROP TABLE IF EXISTS podcast, segment\"\n",
        "with psycopg2.connect(CONNECTION) as conn:\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(DROP_TABLE)\n",
        "    conn.commit() # Commit the changes\n"
      ],
      "metadata": {
        "id": "gpp_3EuU3SN-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Useful function that takes a pd.DataFrame and copies it directly into a table.\n",
        "\n",
        "import pandas as pd\n",
        "import io\n",
        "import psycopg2\n",
        "\n",
        "from typing import List\n",
        "\n",
        "def fast_pg_insert(df: pd.DataFrame, connection: str, table_name: str, columns: List[str]) -> None:\n",
        "    \"\"\"\n",
        "        Inserts data from a pandas DataFrame into a PostgreSQL table using the COPY command for fast insertion.\n",
        "\n",
        "        Parameters:\n",
        "        df (pd.DataFrame): The DataFrame containing the data to be inserted.\n",
        "        connection (str): The connection string to the PostgreSQL database.\n",
        "        table_name (str): The name of the target table in the PostgreSQL database.\n",
        "        columns (List[str]): A list of column names in the target table that correspond to the DataFrame columns.\n",
        "\n",
        "        Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    conn = psycopg2.connect(connection)\n",
        "    _buffer = io.StringIO()\n",
        "    df.to_csv(_buffer, sep=\";\", index=False, header=False)\n",
        "    _buffer.seek(0)\n",
        "    with conn.cursor() as c:\n",
        "        c.copy_from(\n",
        "            file=_buffer,\n",
        "            table=table_name,\n",
        "            sep=\";\",\n",
        "            columns=columns,\n",
        "            null=''\n",
        "        )\n",
        "    conn.commit()\n",
        "    conn.close()"
      ],
      "metadata": {
        "id": "wZDxdvoP4Fov"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Database Schema\n",
        "We will create a database with two tables: podcast and segment:\n",
        "\n",
        "**podcast**\n",
        "\n",
        "- PK: id\n",
        " - The unique podcast id found in the huggingface data (i,e., TRdL6ZzWBS0  is the ID for Jed Buchwald: Isaac Newton and the Philosophy of Science | Lex Fridman Podcast #214)\n",
        "- title\n",
        " - The title of podcast (ie., Jed Buchwald: Isaac Newton and the Philosophy of Science | Lex Fridman Podcast #214)\n",
        "\n",
        "**segment**\n",
        "\n",
        "- PK: id\n",
        " - the unique identifier for the podcast segment. This was created by concatenating the podcast idx and the segment index together (ie., \"0;1\") is the 0th podcast and the 1st segment\n",
        "This is present in the as the \"custom_id\" field in the `embedding.jsonl` and batch_request.jsonl files\n",
        "- start_time\n",
        " - The start timestamp of the segment\n",
        "- end_time\n",
        " - The end timestamp of the segment\n",
        "- content\n",
        " - The raw text transcription of the podcast\n",
        "- embedding\n",
        " - the 128 dimensional vector representation of the text\n",
        "- FK: podcast_id\n",
        " - foreign key to podcast.id"
      ],
      "metadata": {
        "id": "7Y2HkhMZmHFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample document:\n",
        "# {\n",
        "#   \"custom_id\": \"89:115\",\n",
        "#   \"url\": \"/v1/embeddings\",\n",
        "#   \"method\": \"POST\",\n",
        "#   \"body\": {\n",
        "#     \"input\": \" have been possible without these approaches?\",\n",
        "#     \"model\": \"text-embedding-3-large\",\n",
        "#     \"dimensions\": 128,\n",
        "#     \"metadata\": {\n",
        "#       \"title\": \"Podcast: Boris Sofman: Waymo, Cozmo, Self-Driving Cars, and the Future of Robotics | Lex Fridman Podcast #241\",\n",
        "#       \"podcast_id\": \"U_AREIyd0Fc\",\n",
        "#       \"start_time\": 484.52,\n",
        "#       \"stop_time\": 487.08\n",
        "#     }\n",
        "#   }\n",
        "# }\n",
        "\n",
        "# Sample embedding:\n",
        "# {\n",
        "#   \"id\": \"batch_req_QZBmHS7FBiVABxcsGiDx2THJ\",\n",
        "#   \"custom_id\": \"89:115\",\n",
        "#   \"response\": {\n",
        "#     \"status_code\": 200,\n",
        "#     \"request_id\": \"7a55eba082c70aca9e7872d2b694f095\",\n",
        "#     \"body\": {\n",
        "#       \"object\": \"list\",\n",
        "#       \"data\": [\n",
        "#         {\n",
        "#           \"object\": \"embedding\",\n",
        "#           \"index\": 0,\n",
        "#           \"embedding\": [\n",
        "#             0.0035960325,\n",
        "#             126 more lines....\n",
        "#             -0.093248844\n",
        "#           ]\n",
        "#         }\n",
        "#       ],\n",
        "#       \"model\": \"text-embedding-3-large\",\n",
        "#       \"usage\": {\n",
        "#         \"prompt_tokens\": 7,\n",
        "#         \"total_tokens\": 7\n",
        "#       }\n",
        "#     }\n",
        "#   },\n",
        "#   \"error\": null\n",
        "# }"
      ],
      "metadata": {
        "id": "3EZuFdc9m9uP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create table statements that you'll write\n",
        "#TODO\n",
        "\n",
        "# may need to run this to enable vector data type if you didn't select AI in service\n",
        "CREATE_EXTENSION = \"CREATE EXTENSION vector\"\n",
        "\n",
        "# TODO: Add create table statement\n",
        "CREATE_PODCAST_TABLE = \"\"\"\n",
        "CREATE TABLE IF NOT EXISTS podcast (\n",
        "  id VARCHAR(255) PRIMARY KEY,\n",
        "  title TEXT\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "# TODO: Add create table statement\n",
        "CREATE_SEGMENT_TABLE = \"\"\"\n",
        "CREATE TABLE IF NOT EXISTS segment (\n",
        "  id VARCHAR(255) PRIMARY KEY,\n",
        "  start_time FLOAT,\n",
        "  end_time FLOAT,\n",
        "  content TEXT,\n",
        "  embedding VECTOR(128),\n",
        "  podcast_id VARCHAR(255) REFERENCES podcast(id)\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "conn = psycopg2.connect(CONNECTION)\n",
        "# TODO: Create tables with psycopg2 (example: https://www.geeksforgeeks.org/executing-sql-query-with-psycopg2-in-python/)\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# cursor.execute(CREATE_EXTENSION)\n",
        "cursor.execute(CREATE_PODCAST_TABLE)\n",
        "cursor.execute(CREATE_SEGMENT_TABLE)\n",
        "\n",
        "conn.commit()\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "bU6fFAwb5EYO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Extract needed data out of JSONL files. This may be the hard part!\n",
        "\n",
        "# TODO: What data do we need? ids, titles, start and end times, text of segment, embedding vector\n",
        "# TODO: What data is in the documents jsonl files?\n",
        "# id(custom_id), title (metadata.title), start/end time (metadata.start_time or stop_time) text of segment (input)\n",
        "# TODO: What data is in the embedding jsonl files?\n",
        "# the embedding vector (body.data.embedding)\n",
        "# TODO: Get some pandas data frames for our two tables so we can copy the data in!\n",
        "\n",
        "from typing import List\n",
        "\n",
        "documents_dir = \"documents/documents\"\n",
        "embeddings_dir = \"embedding/embedding\"\n",
        "\n",
        "documents = []\n",
        "for file in os.listdir(documents_dir):\n",
        "    if file.endswith(\".jsonl\"):\n",
        "        with open(os.path.join(documents_dir, file), \"r\") as f:\n",
        "            for line in f:\n",
        "                obj = json.loads(line)\n",
        "                documents.append({\n",
        "                    \"segment_id\": obj[\"custom_id\"],\n",
        "                    \"podcast_id\": obj[\"body\"][\"metadata\"][\"podcast_id\"],\n",
        "                    \"title\": obj[\"body\"][\"metadata\"][\"title\"],\n",
        "                    \"start_time\": obj[\"body\"][\"metadata\"][\"start_time\"],\n",
        "                    \"end_time\": obj[\"body\"][\"metadata\"][\"stop_time\"],\n",
        "                    \"content\": obj[\"body\"][\"input\"]\n",
        "                })\n",
        "\n",
        "embedding_dict = {}\n",
        "for file in os.listdir(embeddings_dir):\n",
        "    if file.endswith(\".jsonl\"):\n",
        "        with open(os.path.join(embeddings_dir, file), \"r\") as f:\n",
        "            for line in f:\n",
        "                obj = json.loads(line)\n",
        "                segment_id = obj[\"custom_id\"]\n",
        "                embedding = obj[\"response\"][\"body\"][\"data\"][0][\"embedding\"]\n",
        "                embedding_dict[segment_id] = embedding\n",
        "\n",
        "for doc in documents:\n",
        "    doc[\"embedding\"] = embedding_dict.get(doc[\"segment_id\"])\n",
        "\n",
        "segment_df = pd.DataFrame(documents)\n",
        "podcast_df = (\n",
        "    segment_df[[\"podcast_id\", \"title\"]]\n",
        "    .drop_duplicates()\n",
        "    .rename(columns={\"podcast_id\": \"id\"})\n",
        ")\n",
        "\n",
        "segment_df = segment_df.rename(columns={\"segment_id\": \"id\"})\n",
        "segment_df = segment_df.drop(columns=[\"title\"])\n",
        "\n",
        "segment_df[\"embedding\"] = segment_df[\"embedding\"].apply(json.dumps)\n",
        "\n",
        "print(\"Podcast sample:\")\n",
        "print(podcast_df.head())\n",
        "print(\"\\nSegment sample:\")\n",
        "print(segment_df.head())\n",
        "\n",
        "\n",
        "# Sample document:\n",
        "# {\n",
        "#   \"custom_id\": \"89:115\",\n",
        "#   \"url\": \"/v1/embeddings\",\n",
        "#   \"method\": \"POST\",\n",
        "#   \"body\": {\n",
        "#     \"input\": \" have been possible without these approaches?\",\n",
        "#     \"model\": \"text-embedding-3-large\",\n",
        "#     \"dimensions\": 128,\n",
        "#     \"metadata\": {\n",
        "#       \"title\": \"Podcast: Boris Sofman: Waymo, Cozmo, Self-Driving Cars, and the Future of Robotics | Lex Fridman Podcast #241\",\n",
        "#       \"podcast_id\": \"U_AREIyd0Fc\",\n",
        "#       \"start_time\": 484.52,\n",
        "#       \"stop_time\": 487.08\n",
        "#     }\n",
        "#   }\n",
        "# }\n",
        "\n",
        "# Sample embedding:\n",
        "# {\n",
        "#   \"id\": \"batch_req_QZBmHS7FBiVABxcsGiDx2THJ\",\n",
        "#   \"custom_id\": \"89:115\",\n",
        "#   \"response\": {\n",
        "#     \"status_code\": 200,\n",
        "#     \"request_id\": \"7a55eba082c70aca9e7872d2b694f095\",\n",
        "#     \"body\": {\n",
        "#       \"object\": \"list\",\n",
        "#       \"data\": [\n",
        "#         {\n",
        "#           \"object\": \"embedding\",\n",
        "#           \"index\": 0,\n",
        "#           \"embedding\": [\n",
        "#             0.0035960325,\n",
        "#             126 more lines....\n",
        "#             -0.093248844\n",
        "#           ]\n",
        "#         }\n",
        "#       ],\n",
        "#       \"model\": \"text-embedding-3-large\",\n",
        "#       \"usage\": {\n",
        "#         \"prompt_tokens\": 7,\n",
        "#         \"total_tokens\": 7\n",
        "#       }\n",
        "#     }\n",
        "#   },\n",
        "#   \"error\": null\n",
        "# }\n",
        "\n"
      ],
      "metadata": {
        "id": "v81052OY5BKW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df0381da-c9a8-436d-c78d-dcbe45a651f7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Podcast sample:\n",
            "               id                                              title\n",
            "0     G4hL5Om4IJ4  Podcast: Jim Keller: The Future of Computing, ...\n",
            "3388  zNdhgOk4-fE  Podcast: Silvio Micali: Cryptocurrency, Blockc...\n",
            "4374  q7Qk4vYleXw  Podcast: Lex Fridman Podcast #100 – Alexander ...\n",
            "7390  KW8Vjs84Fxg  Podcast: Ariel Ekblaw: Space Colonization and ...\n",
            "9223  KMgPxVnKLSk  Podcast: Cristiano Amon: Qualcomm CEO | Lex Fr...\n",
            "\n",
            "Segment sample:\n",
            "        id   podcast_id  start_time  end_time  \\\n",
            "0  182:453  G4hL5Om4IJ4     1115.22   1117.14   \n",
            "1  182:454  G4hL5Om4IJ4     1117.14   1121.34   \n",
            "2  182:455  G4hL5Om4IJ4     1121.34   1123.14   \n",
            "3  182:456  G4hL5Om4IJ4     1123.14   1125.94   \n",
            "4  182:457  G4hL5Om4IJ4     1125.94   1128.10   \n",
            "\n",
            "                                             content  \\\n",
            "0        Whereas Intel, when they made their mobile,   \n",
            "1   their foray into mobile, they had one team do...   \n",
            "2                Right? So it wasn't 10 experiments.   \n",
            "3             And then their mindset was PC mindset,   \n",
            "4                        Microsoft software mindset.   \n",
            "\n",
            "                                           embedding  \n",
            "0  [0.14248699, 0.12537144, -0.07477358, -0.05466...  \n",
            "1  [0.0890677, 0.03148626, -0.050297726, -0.12674...  \n",
            "2  [0.09680549, 0.017678397, -0.059745967, 0.0056...  \n",
            "3  [-0.036280353, 0.030522412, -0.07751893, -0.00...  \n",
            "4  [-0.10946793, 0.0009192083, -0.08760755, 0.055...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### Optional #####\n",
        "# In addition to the embedding and document files you might like to load\n",
        "# the full podcast raw data via the hugging face datasets library\n",
        "\n",
        "# from datasets import load_dataset\n",
        "# ds = load_dataset(\"Whispering-GPT/lex-fridman-podcast\")\n"
      ],
      "metadata": {
        "id": "xo3Y8IHYRruE",
        "outputId": "0fe98408-3a60-42ec-fec9-bd77160c4ea7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before deduplication: 346\n",
            "Unique podcast IDs: 346\n",
            "After deduplication: 346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO Copy all the \"podcast\" data into the podcast postgres table!\n",
        "fast_pg_insert(podcast_df, CONNECTION, \"podcast\", [\"id\", \"title\"])"
      ],
      "metadata": {
        "id": "5W3f-2iTpGL0"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO Copy all the \"segment\" data into the segment postgres table!\n",
        "# HINT 1: use the recommender.utils.fast_pg_insert function to insert data into the database\n",
        "# otherwise inserting the 800k documents will take a very, very long time\n",
        "# HINT 2: if you don't want to use all your memory and crash\n",
        "# colab, you'll need to either send the data up in chunks\n",
        "# or write your own function for copying it up. Alternative to chunking maybe start\n",
        "# with writing it to a CSV and then copy it up?\n",
        "import numpy as np\n",
        "\n",
        "def chunk_and_insert(df: pd.DataFrame, connection_string: str, table_name: str, columns: list, chunk_size: int = 5000):\n",
        "    num_chunks = int(np.ceil(len(df) / chunk_size))\n",
        "    for i in range(num_chunks):\n",
        "        chunk_df = df.iloc[i * chunk_size: (i + 1) * chunk_size]\n",
        "        fast_pg_insert(chunk_df, connection_string, table_name, columns)\n",
        "        print(f\"Inserted chunk {i + 1}/{num_chunks} into {table_name}\")\n",
        "\n",
        "chunk_and_insert(segment_df, CONNECTION, \"segment\", [\n",
        "    \"id\", \"podcast_id\", \"start_time\", \"end_time\", \"content\", \"embedding\"\n",
        "])"
      ],
      "metadata": {
        "id": "ZTUsciGfpahF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00529a0c-8204-468d-cecf-3ebd601ebb0c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inserted chunk 1/167 into segment\n",
            "Inserted chunk 2/167 into segment\n",
            "Inserted chunk 3/167 into segment\n",
            "Inserted chunk 4/167 into segment\n",
            "Inserted chunk 5/167 into segment\n",
            "Inserted chunk 6/167 into segment\n",
            "Inserted chunk 7/167 into segment\n",
            "Inserted chunk 8/167 into segment\n",
            "Inserted chunk 9/167 into segment\n",
            "Inserted chunk 10/167 into segment\n",
            "Inserted chunk 11/167 into segment\n",
            "Inserted chunk 12/167 into segment\n",
            "Inserted chunk 13/167 into segment\n",
            "Inserted chunk 14/167 into segment\n",
            "Inserted chunk 15/167 into segment\n",
            "Inserted chunk 16/167 into segment\n",
            "Inserted chunk 17/167 into segment\n",
            "Inserted chunk 18/167 into segment\n",
            "Inserted chunk 19/167 into segment\n",
            "Inserted chunk 20/167 into segment\n",
            "Inserted chunk 21/167 into segment\n",
            "Inserted chunk 22/167 into segment\n",
            "Inserted chunk 23/167 into segment\n",
            "Inserted chunk 24/167 into segment\n",
            "Inserted chunk 25/167 into segment\n",
            "Inserted chunk 26/167 into segment\n",
            "Inserted chunk 27/167 into segment\n",
            "Inserted chunk 28/167 into segment\n",
            "Inserted chunk 29/167 into segment\n",
            "Inserted chunk 30/167 into segment\n",
            "Inserted chunk 31/167 into segment\n",
            "Inserted chunk 32/167 into segment\n",
            "Inserted chunk 33/167 into segment\n",
            "Inserted chunk 34/167 into segment\n",
            "Inserted chunk 35/167 into segment\n",
            "Inserted chunk 36/167 into segment\n",
            "Inserted chunk 37/167 into segment\n",
            "Inserted chunk 38/167 into segment\n",
            "Inserted chunk 39/167 into segment\n",
            "Inserted chunk 40/167 into segment\n",
            "Inserted chunk 41/167 into segment\n",
            "Inserted chunk 42/167 into segment\n",
            "Inserted chunk 43/167 into segment\n",
            "Inserted chunk 44/167 into segment\n",
            "Inserted chunk 45/167 into segment\n",
            "Inserted chunk 46/167 into segment\n",
            "Inserted chunk 47/167 into segment\n",
            "Inserted chunk 48/167 into segment\n",
            "Inserted chunk 49/167 into segment\n",
            "Inserted chunk 50/167 into segment\n",
            "Inserted chunk 51/167 into segment\n",
            "Inserted chunk 52/167 into segment\n",
            "Inserted chunk 53/167 into segment\n",
            "Inserted chunk 54/167 into segment\n",
            "Inserted chunk 55/167 into segment\n",
            "Inserted chunk 56/167 into segment\n",
            "Inserted chunk 57/167 into segment\n",
            "Inserted chunk 58/167 into segment\n",
            "Inserted chunk 59/167 into segment\n",
            "Inserted chunk 60/167 into segment\n",
            "Inserted chunk 61/167 into segment\n",
            "Inserted chunk 62/167 into segment\n",
            "Inserted chunk 63/167 into segment\n",
            "Inserted chunk 64/167 into segment\n",
            "Inserted chunk 65/167 into segment\n",
            "Inserted chunk 66/167 into segment\n",
            "Inserted chunk 67/167 into segment\n",
            "Inserted chunk 68/167 into segment\n",
            "Inserted chunk 69/167 into segment\n",
            "Inserted chunk 70/167 into segment\n",
            "Inserted chunk 71/167 into segment\n",
            "Inserted chunk 72/167 into segment\n",
            "Inserted chunk 73/167 into segment\n",
            "Inserted chunk 74/167 into segment\n",
            "Inserted chunk 75/167 into segment\n",
            "Inserted chunk 76/167 into segment\n",
            "Inserted chunk 77/167 into segment\n",
            "Inserted chunk 78/167 into segment\n",
            "Inserted chunk 79/167 into segment\n",
            "Inserted chunk 80/167 into segment\n",
            "Inserted chunk 81/167 into segment\n",
            "Inserted chunk 82/167 into segment\n",
            "Inserted chunk 83/167 into segment\n",
            "Inserted chunk 84/167 into segment\n",
            "Inserted chunk 85/167 into segment\n",
            "Inserted chunk 86/167 into segment\n",
            "Inserted chunk 87/167 into segment\n",
            "Inserted chunk 88/167 into segment\n",
            "Inserted chunk 89/167 into segment\n",
            "Inserted chunk 90/167 into segment\n",
            "Inserted chunk 91/167 into segment\n",
            "Inserted chunk 92/167 into segment\n",
            "Inserted chunk 93/167 into segment\n",
            "Inserted chunk 94/167 into segment\n",
            "Inserted chunk 95/167 into segment\n",
            "Inserted chunk 96/167 into segment\n",
            "Inserted chunk 97/167 into segment\n",
            "Inserted chunk 98/167 into segment\n",
            "Inserted chunk 99/167 into segment\n",
            "Inserted chunk 100/167 into segment\n",
            "Inserted chunk 101/167 into segment\n",
            "Inserted chunk 102/167 into segment\n",
            "Inserted chunk 103/167 into segment\n",
            "Inserted chunk 104/167 into segment\n",
            "Inserted chunk 105/167 into segment\n",
            "Inserted chunk 106/167 into segment\n",
            "Inserted chunk 107/167 into segment\n",
            "Inserted chunk 108/167 into segment\n",
            "Inserted chunk 109/167 into segment\n",
            "Inserted chunk 110/167 into segment\n",
            "Inserted chunk 111/167 into segment\n",
            "Inserted chunk 112/167 into segment\n",
            "Inserted chunk 113/167 into segment\n",
            "Inserted chunk 114/167 into segment\n",
            "Inserted chunk 115/167 into segment\n",
            "Inserted chunk 116/167 into segment\n",
            "Inserted chunk 117/167 into segment\n",
            "Inserted chunk 118/167 into segment\n",
            "Inserted chunk 119/167 into segment\n",
            "Inserted chunk 120/167 into segment\n",
            "Inserted chunk 121/167 into segment\n",
            "Inserted chunk 122/167 into segment\n",
            "Inserted chunk 123/167 into segment\n",
            "Inserted chunk 124/167 into segment\n",
            "Inserted chunk 125/167 into segment\n",
            "Inserted chunk 126/167 into segment\n",
            "Inserted chunk 127/167 into segment\n",
            "Inserted chunk 128/167 into segment\n",
            "Inserted chunk 129/167 into segment\n",
            "Inserted chunk 130/167 into segment\n",
            "Inserted chunk 131/167 into segment\n",
            "Inserted chunk 132/167 into segment\n",
            "Inserted chunk 133/167 into segment\n",
            "Inserted chunk 134/167 into segment\n",
            "Inserted chunk 135/167 into segment\n",
            "Inserted chunk 136/167 into segment\n",
            "Inserted chunk 137/167 into segment\n",
            "Inserted chunk 138/167 into segment\n",
            "Inserted chunk 139/167 into segment\n",
            "Inserted chunk 140/167 into segment\n",
            "Inserted chunk 141/167 into segment\n",
            "Inserted chunk 142/167 into segment\n",
            "Inserted chunk 143/167 into segment\n",
            "Inserted chunk 144/167 into segment\n",
            "Inserted chunk 145/167 into segment\n",
            "Inserted chunk 146/167 into segment\n",
            "Inserted chunk 147/167 into segment\n",
            "Inserted chunk 148/167 into segment\n",
            "Inserted chunk 149/167 into segment\n",
            "Inserted chunk 150/167 into segment\n",
            "Inserted chunk 151/167 into segment\n",
            "Inserted chunk 152/167 into segment\n",
            "Inserted chunk 153/167 into segment\n",
            "Inserted chunk 154/167 into segment\n",
            "Inserted chunk 155/167 into segment\n",
            "Inserted chunk 156/167 into segment\n",
            "Inserted chunk 157/167 into segment\n",
            "Inserted chunk 158/167 into segment\n",
            "Inserted chunk 159/167 into segment\n",
            "Inserted chunk 160/167 into segment\n",
            "Inserted chunk 161/167 into segment\n",
            "Inserted chunk 162/167 into segment\n",
            "Inserted chunk 163/167 into segment\n",
            "Inserted chunk 164/167 into segment\n",
            "Inserted chunk 165/167 into segment\n",
            "Inserted chunk 166/167 into segment\n",
            "Inserted chunk 167/167 into segment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## This script is used to query the database\n",
        "import os\n",
        "import psycopg2\n",
        "\n",
        "\n",
        "# Write your queries\n",
        "# Q1) What are the five most similar segments to segment \"267:476\"\n",
        "# Input: \"that if we were to meet alien life at some point\"\n",
        "# For each result return the podcast name, the segment id, segment raw text,  the start time, stop time, and embedding distance\n",
        "\n",
        "conn = psycopg2.connect(CONNECTION)\n",
        "cur = conn.cursor()\n",
        "cur.execute(\"\"\"\n",
        "with prompt_embedding as (\n",
        "  select embedding\n",
        "  from segment\n",
        "  where id = '267:476'\n",
        ")\n",
        "select podcast.title,\n",
        "       segment.id,\n",
        "       segment.content,\n",
        "       segment.start_time,\n",
        "       segment.end_time,\n",
        "       segment.embedding <-> (SELECT embedding FROM prompt_embedding) as distance\n",
        "from segment\n",
        "join podcast on segment.podcast_id = podcast.id\n",
        "where segment.id != '267:476'\n",
        "order by distance\n",
        "limit 5\n",
        "\"\"\")\n",
        "for row in cur.fetchall():\n",
        "  print(row)\n",
        "\n",
        "conn.commit()\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "NvkG-51G5IDe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2020378-2873-488e-c818-b0de38539b5d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Podcast: Ryan Graves: UFOs, Fighter Jets, and Aliens | Lex Fridman Podcast #308', '113:2792', ' encounters, human beings, if we were to meet another alien', 6725.62, 6729.86, 0.6483450674336982)\n",
            "('Podcast: Richard Dawkins: Evolution, Intelligence, Simulation, and Memes | Lex Fridman Podcast #87', '268:1019', ' Suppose we did meet an alien from outer space', 2900.04, 2903.0800000000004, 0.6558106859320757)\n",
            "('Podcast: Jeffrey Shainline: Neuromorphic Computing and Optoelectronic Intelligence | Lex Fridman Podcast #225', '305:3600', ' but if we think of alien civilizations out there', 9479.960000000001, 9484.04, 0.6595433115268592)\n",
            "('Podcast: Michio Kaku: Future of Humans, Aliens, Space Travel & Physics | Lex Fridman Podcast #45', '18:464', ' So I think when we meet alien life from outer space,', 1316.8600000000001, 1319.5800000000002, 0.6662026419636159)\n",
            "('Podcast: Alien Debate: Sara Walker and Lee Cronin | Lex Fridman Podcast #279', '71:989', ' because if aliens come to us', 2342.34, 2343.6200000000003, 0.6742942635162208)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2) What are the five most dissimilar segments to segment \"267:476\"\n",
        "# Input: \"that if we were to meet alien life at some point\"\n",
        "# For each result return the podcast name, the segment id, segment raw text, the start time, stop time, and embedding distance\n",
        "\n",
        "conn = psycopg2.connect(CONNECTION)\n",
        "cur = conn.cursor()\n",
        "cur.execute(\"\"\"\n",
        "with prompt_embedding as (\n",
        "  select embedding\n",
        "  from segment\n",
        "  where id = '267:476'\n",
        ")\n",
        "select podcast.title,\n",
        "       segment.id,\n",
        "       segment.content,\n",
        "       segment.start_time,\n",
        "       segment.end_time,\n",
        "       segment.embedding <-> (SELECT embedding FROM prompt_embedding) as distance\n",
        "from segment\n",
        "join podcast on segment.podcast_id = podcast.id\n",
        "where segment.id != '267:476'\n",
        "order by distance desc\n",
        "limit 5\n",
        "\"\"\")\n",
        "for row in cur.fetchall():\n",
        "  print(row)\n",
        "\n",
        "conn.commit()\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "Dq8ePSfrw8Ix",
        "outputId": "973633c4-77b0-4df6-ed0d-e76ecf4f45fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Podcast: Jason Calacanis: Startups, Angel Investing, Capitalism, and Friendship | Lex Fridman Podcast #161', '119:218', ' a 73 Mustang Grande in gold?', 519.96, 523.8000000000001, 1.6157687685840119)\n",
            "('Podcast: Rana el Kaliouby: Emotion AI, Social Robots, and Self-Driving Cars | Lex Fridman Podcast #322', '133:2006', ' for 94 car models.', 5818.62, 5820.82, 1.5863359073014982)\n",
            "('Podcast: Travis Stevens: Judo, Olympics, and Mental Toughness | Lex Fridman Podcast #223', '283:1488', ' when I called down to get the sauna.', 3709.34, 3711.1000000000004, 1.572552805197421)\n",
            "('Podcast: Jeremy Howard: fast.ai Deep Learning Courses and Research | Lex Fridman Podcast #35', '241:1436', ' which has all the courses pre-installed.', 4068.9, 4071.1400000000003, 1.5663319710412156)\n",
            "('Podcast: Joscha Bach: Nature of Reality, Dreams, and Consciousness | Lex Fridman Podcast #212', '307:3933', ' and very few are first class and some are budget.', 10648.64, 10650.960000000001, 1.5616341289820461)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3) What are the five most similar segments to segment '48:511'\n",
        "\n",
        "# Input: \"Is it is there something especially interesting and profound to you in terms of our current deep learning neural network, artificial neural network approaches and the whatever we do understand about the biological neural network.\"\n",
        "# For each result return the podcast name, the segment id, segment raw text,  the start time, stop time, and embedding distance\n",
        "\n",
        "\n",
        "conn = psycopg2.connect(CONNECTION)\n",
        "cur = conn.cursor()\n",
        "cur.execute(\"\"\"\n",
        "with prompt_embedding as (\n",
        "  select embedding\n",
        "  from segment\n",
        "  where id = '48:511'\n",
        ")\n",
        "select podcast.title,\n",
        "       segment.id,\n",
        "       segment.content,\n",
        "       segment.start_time,\n",
        "       segment.end_time,\n",
        "       segment.embedding <-> (SELECT embedding FROM prompt_embedding) as distance\n",
        "from segment\n",
        "join podcast on segment.podcast_id = podcast.id\n",
        "where segment.id != '48:511'\n",
        "order by distance\n",
        "limit 5\n",
        "\"\"\")\n",
        "for row in cur.fetchall():\n",
        "  print(row)\n",
        "\n",
        "conn.commit()\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "dmTK02bZk3pF",
        "outputId": "8c8b1e24-2dcf-4fab-b1ae-a793665df3e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Podcast: Andrew Huberman: Neuroscience of Optimal Performance | Lex Fridman Podcast #139', '155:648', ' Is there something interesting to you or fundamental to you about the circuitry of the brain', 3798.48, 3805.84, 0.652299685331962)\n",
            "('Podcast: Cal Newport: Deep Work, Focus, Productivity, Email, and Social Media | Lex Fridman Podcast #166', '61:3707', ' of what we might discover about neural networks?', 8498.02, 8500.1, 0.7121050124628524)\n",
            "('Podcast: Matt Botvinick: Neuroscience, Psychology, and AI at DeepMind | Lex Fridman Podcast #106', '48:512', \" And our brain is there. There's some there's quite a few differences. Are some of them to you either interesting or perhaps profound in terms of in terms of the gap we might want to try to close in trying to create a human level intelligence.\", 1846.84, 1865.84, 0.7195603322334674)\n",
            "('Podcast: Yann LeCun: Dark Matter of Intelligence and Self-Supervised Learning | Lex Fridman Podcast #258', '276:2642', ' Have these, I mean, small pockets of beautiful complexity. Does that, do cellular automata, do these kinds of emergence and complex systems give you some intuition or guide your understanding of machine learning systems and neural networks and so on?', 8628.16, 8646.16, 0.7357217735737499)\n",
            "('Podcast: Stephen Wolfram: Fundamental Theory of Physics, Life, and the Universe | Lex Fridman Podcast #124', '2:152', ' So is there something like that with physics where so deep learning neural networks have been around for a long time?', 610.86, 618.86, 0.7366969553372291)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4) What are the five most similar segments to segment '51:56'\n",
        "\n",
        "# Input: \"But what about like the fundamental physics of dark energy? Is there any understanding of what the heck it is?\"\n",
        "# For each result return the podcast name, the segment id, segment raw text,  the start time, stop time, and embedding distance\n",
        "\n",
        "conn = psycopg2.connect(CONNECTION)\n",
        "cur = conn.cursor()\n",
        "cur.execute(\"\"\"\n",
        "with prompt_embedding as (\n",
        "  select embedding\n",
        "  from segment\n",
        "  where id = '51:56'\n",
        ")\n",
        "select podcast.title,\n",
        "       segment.id,\n",
        "       segment.content,\n",
        "       segment.start_time,\n",
        "       segment.end_time,\n",
        "       segment.embedding <-> (SELECT embedding FROM prompt_embedding) as distance\n",
        "from segment\n",
        "join podcast on segment.podcast_id = podcast.id\n",
        "where segment.id != '51:56'\n",
        "order by distance\n",
        "limit 5\n",
        "\"\"\")\n",
        "for row in cur.fetchall():\n",
        "  print(row)\n",
        "\n",
        "conn.commit()\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "jcfhAKKQk9rV",
        "outputId": "b73faf1c-5723-4a91-ce89-5040cec988d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Podcast: George Hotz: Hacking the Simulation & Learning to Drive with Neural Nets | Lex Fridman Podcast #132', '308:144', \" I mean, we don't understand dark energy, right?\", 500.44, 502.6, 0.6681965222094363)\n",
            "('Podcast: Lex Fridman: Ask Me Anything - AMA January 2021 | Lex Fridman Podcast', '243:273', \" Like, what's up with this dark matter and dark energy stuff?\", 946.22, 950.12, 0.7355511762966292)\n",
            "('Podcast: Katherine de Kleer: Planets, Moons, Asteroids & Life in Our Solar System | Lex Fridman Podcast #184', '196:685', ' being like, what the hell is dark matter and dark energy?', 2591.72, 2595.9599999999996, 0.7631141596843518)\n",
            "('Podcast: Alex Filippenko: Supernovae, Dark Energy, Aliens & the Expanding Universe | Lex Fridman Podcast #137', '51:36', ' Do we have any understanding of what the heck that thing is?', 216.0, 219.0, 0.7922019445543276)\n",
            "('Podcast: Leonard Susskind: Quantum Mechanics, String Theory and Black Holes | Lex Fridman Podcast #41', '122:831', ' That is a big question in physics right now.', 2374.9, 2377.6200000000003, 0.8022704628640559)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q5) For each of the following podcast segments, find the five most similar podcast episodes. Hint: You can do this by averaging over the embedding vectors within a podcast episode.\n",
        "\n",
        "\n",
        "conn = psycopg2.connect(CONNECTION)\n",
        "cur = conn.cursor()\n",
        "\n",
        "#     a) Segment \"267:476\"\n",
        "cur.execute(\"\"\"\n",
        "with prompt_podcast as (\n",
        "  select podcast_id\n",
        "  from segment\n",
        "  where id = '267:476'\n",
        "), avg_prompt_embedding as (\n",
        "  select avg(embedding) as avg_embedding\n",
        "  from segment\n",
        "  where podcast_id = (select podcast_id from prompt_podcast)\n",
        ")\n",
        "select podcast.title,\n",
        "       avg(segment.embedding) <-> (SELECT avg_embedding FROM avg_prompt_embedding) as distance\n",
        "from segment\n",
        "join podcast on segment.podcast_id = podcast.id\n",
        "where segment.id != '267:476'\n",
        "group by podcast.id\n",
        "order by distance\n",
        "limit 5\n",
        "\"\"\")\n",
        "print(\"A\")\n",
        "for row in cur.fetchall():\n",
        "  print(row)\n",
        "\n",
        "#     b) Segment '48:511'\n",
        "cur.execute(\"\"\"\n",
        "with prompt_embedding as (\n",
        "  select embedding\n",
        "  from segment\n",
        "  where id = '48:511'\n",
        ")\n",
        "select podcast.title,\n",
        "       segment.embedding <-> (SELECT embedding FROM prompt_embedding) as distance\n",
        "from segment\n",
        "join podcast on segment.podcast_id = podcast.id\n",
        "where segment.id != '48:511'\n",
        "order by distance\n",
        "limit 5\n",
        "\"\"\")\n",
        "print(\"B\")\n",
        "for row in cur.fetchall():\n",
        "  print(row)\n",
        "\n",
        "#     c) Segment '51:56'\n",
        "cur.execute(\"\"\"\n",
        "with prompt_embedding as (\n",
        "  select embedding\n",
        "  from segment\n",
        "  where id = '51:56'\n",
        ")\n",
        "select podcast.title,\n",
        "       segment.embedding <-> (SELECT embedding FROM prompt_embedding) as distance\n",
        "from segment\n",
        "join podcast on segment.podcast_id = podcast.id\n",
        "where segment.id != '51:56'\n",
        "order by distance\n",
        "limit 5\n",
        "\"\"\")\n",
        "print(\"C\")\n",
        "for row in cur.fetchall():\n",
        "  print(row)\n",
        "\n",
        "# For each result return the Podcast title and the embedding distance\n",
        "\n",
        "conn.commit()\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "OT4yTTn4k_iX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc71cbdc-f965-447d-f274-5afe9c6c2b5f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A\n",
            "('Podcast: David Silver: AlphaGo, AlphaZero, and Deep Reinforcement Learning | Lex Fridman Podcast #86', 0.0004139695643608583)\n",
            "('Podcast: Demis Hassabis: DeepMind - AI, Superintelligence & the Future of Humanity | Lex Fridman Podcast #299', 0.11301193616129587)\n",
            "('Podcast: Oriol Vinyals: DeepMind AlphaStar, StarCraft, and Language | Lex Fridman Podcast #20', 0.11327739413045972)\n",
            "('Podcast: Ilya Sutskever: Deep Learning | Lex Fridman Podcast #94', 0.12476937431692706)\n",
            "('Podcast: Greg Brockman: OpenAI and AGI | Lex Fridman Podcast #17', 0.13182197779463417)\n",
            "B\n",
            "('Podcast: Andrew Huberman: Neuroscience of Optimal Performance | Lex Fridman Podcast #139', 0.652299685331962)\n",
            "('Podcast: Cal Newport: Deep Work, Focus, Productivity, Email, and Social Media | Lex Fridman Podcast #166', 0.7121050124628524)\n",
            "('Podcast: Matt Botvinick: Neuroscience, Psychology, and AI at DeepMind | Lex Fridman Podcast #106', 0.7195603322334674)\n",
            "('Podcast: Yann LeCun: Dark Matter of Intelligence and Self-Supervised Learning | Lex Fridman Podcast #258', 0.7357217735737499)\n",
            "('Podcast: Stephen Wolfram: Fundamental Theory of Physics, Life, and the Universe | Lex Fridman Podcast #124', 0.7366969553372291)\n",
            "C\n",
            "('Podcast: George Hotz: Hacking the Simulation & Learning to Drive with Neural Nets | Lex Fridman Podcast #132', 0.6681965222094363)\n",
            "('Podcast: Lex Fridman: Ask Me Anything - AMA January 2021 | Lex Fridman Podcast', 0.7355511762966292)\n",
            "('Podcast: Katherine de Kleer: Planets, Moons, Asteroids & Life in Our Solar System | Lex Fridman Podcast #184', 0.7631141596843518)\n",
            "('Podcast: Alex Filippenko: Supernovae, Dark Energy, Aliens & the Expanding Universe | Lex Fridman Podcast #137', 0.7922019445543276)\n",
            "('Podcast: Leonard Susskind: Quantum Mechanics, String Theory and Black Holes | Lex Fridman Podcast #41', 0.8022704628640559)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q6) For podcast episode id = VeH7qKZr0WI, find the five most similar podcast episodes. Hint: you can do a similar averaging procedure as Q5\n",
        "\n",
        "# Input Episode: \"Balaji Srinivasan: How to Fix Government, Twitter, Science, and the FDA | Lex Fridman Podcast #331\"\n",
        "# For each result return the Podcast title and the embedding distance\n"
      ],
      "metadata": {
        "id": "_oKIVjn4lBYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deliverables\n",
        "You will turn in a ZIP or PDF file containing all your code and a PDF file with the queries and results for questions 1-7."
      ],
      "metadata": {
        "id": "WBZVtZP4lDO2"
      }
    }
  ]
}